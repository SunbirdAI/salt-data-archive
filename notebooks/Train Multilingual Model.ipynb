{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13b06e57",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce02158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "!pip install transformers\n",
    "!pip install sacrebleu\n",
    "!pip install sacremoses\n",
    "!pip install datasets\n",
    "!pip install wandb\n",
    "!pip install sentencepiece\n",
    "display.clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36461e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from IPython import display\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import sentencepiece\n",
    "import sacrebleu\n",
    "import sacremoses\n",
    "import tqdm\n",
    "import transformers\n",
    "import torch\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c875def4-aa4a-46b1-ac1b-71e8702dcdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea76c66",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Alternatives for pre-training when translating to English: `Helsinki-NLP/opus-mt-lg-en`, `Helsinki-NLP/opus-mt-mul-en`.\n",
    "\n",
    "Note 1: when training on V100 GPUs, there is more memory and `train_batch_size` can be increased (to 64?). If this is done then `gradient_accumulation_steps` should then be decreased accordingly, so that there is the same effective batch size.\n",
    "\n",
    "Note 2: there is little difference in BLEU score when using a test set of 500 vs 1000 sentences per language. For rapid parameter tuning, we can therefore use `config['validation_samples_per_language'] = 500`, and then set it to 1000 for the best model config to report numbers in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b27659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for mul-en models\n",
    "config = {\n",
    "    'source_language': 'mul',\n",
    "    'target_language': 'en',\n",
    "    'metric_for_best_model': 'loss',\n",
    "    'train_batch_size': 20,\n",
    "    'gradient_accumulation_steps': 150,\n",
    "    'max_input_length': 128,\n",
    "    'max_target_length': 128,\n",
    "    'validation_samples_per_language': 500,\n",
    "    'eval_batch_size': 16,\n",
    "    'eval_languages': [\"ach\", \"lgg\", \"lug\", \"nyn\", \"teo\"],\n",
    "    'eval_pretrained_model': False,\n",
    "    'learning_rate': 1e-4,\n",
    "    'num_train_epochs': 10,\n",
    "    'label_smoothing_factor': 0.1,\n",
    "    'flores101_training_data': True,\n",
    "    'mt560_training_data': True,\n",
    "    'back_translation_training_data': True,\n",
    "    'named_entities_training_data': True,\n",
    "}\n",
    "\n",
    "config['language_pair'] = f'{config[\"source_language\"]}-{config[\"target_language\"]}'\n",
    "config['wandb_project'] = f'sunbird-translate-{config[\"language_pair\"]}'\n",
    "config['model_checkpoint'] = f'Helsinki-NLP/opus-mt-{config[\"language_pair\"]}'\n",
    "\n",
    "# What training data to use\n",
    "config['data_dir'] = f'v7-dataset/v7.0/supervised/{config[\"language_pair\"]}/'\n",
    "\n",
    "# Evaluate roughly every 10 minutes\n",
    "eval_steps_interval = 350 * 60 * 7 / (config['gradient_accumulation_steps']\n",
    "                                      * config['train_batch_size'])\n",
    "\n",
    "eval_steps_interval = 10 * max(1, int(eval_steps_interval / 10))\n",
    "\n",
    "print(f'Evaluating every {eval_steps_interval} training steps.')\n",
    "\n",
    "config['train_settings'] = transformers.Seq2SeqTrainingArguments(\n",
    "    f'output-{config[\"language_pair\"]}',\n",
    "    evaluation_strategy = 'steps',\n",
    "    eval_steps = eval_steps_interval,\n",
    "    save_steps = eval_steps_interval,\n",
    "    gradient_accumulation_steps = config['gradient_accumulation_steps'],\n",
    "    learning_rate = config['learning_rate'],\n",
    "    per_device_train_batch_size = config['train_batch_size'],\n",
    "    per_device_eval_batch_size = config['eval_batch_size'],\n",
    "    weight_decay = 0.01,\n",
    "    save_total_limit = 3,\n",
    "    num_train_epochs = config['num_train_epochs'],\n",
    "    predict_with_generate = True,\n",
    "    fp16 = torch.cuda.is_available(),\n",
    "    logging_dir = f'output-{config[\"language_pair\"]}',\n",
    "    report_to = 'wandb',\n",
    "    run_name = f'{config[\"source_language\"]}-{config[\"target_language\"]}',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model = config['metric_for_best_model'],\n",
    "    label_smoothing_factor = config['label_smoothing_factor']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dade6a8-9f93-4d64-8fec-e4462bf425e9",
   "metadata": {},
   "source": [
    "MT560 is much bigger than the other training sets, so oversample the rest (by 5x) to balance it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b7ae1b-0675-4c78-bf9a-22d40ae9f930",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['training_subset_ids'] = [\n",
    "    'train', 'train_ai4d',\n",
    "    'val_ach', 'val_lgg', 'val_lug', 'val_nyn', 'val_teo',\n",
    "]\n",
    "\n",
    "if config['flores101_training_data']:\n",
    "    config['training_subset_ids'] .append('train_flores_lug')\n",
    "\n",
    "if config['back_translation_training_data']:\n",
    "    config['training_subset_ids'].append('back_translated')\n",
    "\n",
    "# Over-sample the non-religious training text\n",
    "config['training_subset_ids'] = config['training_subset_ids'] * 5\n",
    "\n",
    "if config['mt560_training_data']:\n",
    "    config['training_subset_ids'].extend([\n",
    "        'train_mt560_lug', 'train_mt560_ach', 'train_mt560_nyn',\n",
    "    ])\n",
    "\n",
    "if config['named_entities_training_data']:\n",
    "    config['training_subset_ids'].append('named_entities')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f607807-40be-4cda-8fa7-67abb72f0234",
   "metadata": {},
   "source": [
    "# Set up datasets\n",
    "\n",
    "Download the raw text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dcda22-59ab-49b3-b2fd-5a8b200dead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('v7-dataset'):\n",
    "    !wget https://sunbird-translate.s3.us-east-2.amazonaws.com/v7-dataset.zip\n",
    "    !unzip v7-dataset.zip\n",
    "    display.clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2332961c-fb4b-4249-bac1-2c8e4384faa2",
   "metadata": {},
   "source": [
    "Create a training set by interleaving separate training subsets.\n",
    "\n",
    "Notes:\n",
    "* This includes MT560 which has many examples (484,925), but which is biased towards religious text so we sample from it sparsely.\n",
    "* We just use a 2-way train/test split for this experiment, so include the validation sentences in with the training set.\n",
    "* LGG, ACH and TEO are oversampled a little by duplicating the validation sets, as a simple way to correct for there being more LUG and NYN training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db95e17-5b8a-473f-b38f-2303ca7d9f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _file_to_list(path):\n",
    "    with open(path) as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [line.rstrip() for line in lines]\n",
    "        return lines\n",
    "    \n",
    "def dataset_from_src_tgt_files(data_dir, dataset_id, read_first_n = 0):\n",
    "    path = os.path.join(data_dir, dataset_id)\n",
    "    source, target = [_file_to_list(path + '.src'),\n",
    "                      _file_to_list(path + '.tgt')]\n",
    "    if read_first_n:\n",
    "        source = source[:read_first_n]\n",
    "        target = target[:read_first_n]\n",
    "    pairs = {'translation': [{config['source_language']: s,\n",
    "                              config['target_language']: t}\n",
    "                             for s, t in zip(source, target)]}\n",
    "    return datasets.Dataset.from_dict(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50887e34-b0c7-40d2-8e00-2cfa525a3e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_subsets = [dataset_from_src_tgt_files(config['data_dir'], id)\n",
    "                    for id in config['training_subset_ids']]\n",
    "training_subsets = [s.shuffle() for s in training_subsets]\n",
    "\n",
    "sample_probabilities = np.array([len(s) for s in training_subsets])\n",
    "sample_probabilities = sample_probabilities / np.sum(sample_probabilities)\n",
    "\n",
    "train_data_raw = datasets.interleave_datasets(\n",
    "    training_subsets, sample_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd838152-63c9-45ef-830d-564d6d34b72f",
   "metadata": {},
   "source": [
    "Make the separate validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e273c90-378b-46fd-b07b-52a3caa3725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_subsets = [dataset_from_src_tgt_files(\n",
    "    config['data_dir'], f'test_{lang}', read_first_n = config['validation_samples_per_language'])\n",
    "    for lang in config['eval_languages']]\n",
    "validation_data_raw = datasets.concatenate_datasets(validation_subsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b26f9",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "Note that whatever pre-processing we do here (punctuation normalisation and ensuring sentence case), we should also do at test-time when running the model on real queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda8c8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_format(input):\n",
    "    '''Ensure capital letter at the start and full stop at the end.'''\n",
    "    input = input[0].capitalize() + input[1:]\n",
    "    if input[-1] not in ['.', '!', '?']:\n",
    "        input = input + '.'\n",
    "    return input\n",
    "\n",
    "def preprocess(examples):\n",
    "    normalizer = sacremoses.MosesPunctNormalizer()\n",
    "    \n",
    "    inputs = [ex[config['source_language']] for ex in examples['translation']]\n",
    "    targets = [ex[config['target_language']] for ex in examples['translation']]\n",
    "\n",
    "    inputs = [sentence_format(normalizer.normalize(text))\n",
    "              for text in inputs]\n",
    "    targets = [sentence_format(normalizer.normalize(text))\n",
    "               for text in targets]\n",
    "    \n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=config['max_input_length'], truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            targets, max_length=config['max_target_length'], truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "def postprocess(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds, eval_languages, samples_per_language):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "        \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess(decoded_preds, decoded_labels)\n",
    "    \n",
    "    result = {}\n",
    "    for i, lang in enumerate(eval_languages):\n",
    "        result_subset = metric.compute(\n",
    "            predictions=decoded_preds[i*samples_per_language:(i+1)*samples_per_language],\n",
    "            references=decoded_labels[i*samples_per_language:(i+1)*samples_per_language])\n",
    "        result[f\"BLEU_{lang}\"] = result_subset[\"score\"]\n",
    "        \n",
    "    result[\"BLEU_mean\"] = np.mean([result[f\"BLEU_{lang}\"] for lang in eval_languages])\n",
    "    \n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a392ae",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Instantiate the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59977cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(config['model_checkpoint'])\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(config['model_checkpoint'])\n",
    "data_collator = transformers.DataCollatorForSeq2Seq(tokenizer, model = model) \n",
    "metric = datasets.load_metric('sacrebleu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f7e240-2824-4862-80e0-5dbb4c0de094",
   "metadata": {},
   "source": [
    "For multiple language outputs, we need to make sure the language codes have some mapping in the encoder. We can re-use the token indices of some other language codes in the pre-trained model that we don't need.\n",
    "\n",
    "In `Helsinki-NLP/opus-mt-en-mul`, only Luganda (`lug`) is already supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c9a37-c7a7-4954-a6d7-fe3d44dfb1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['target_language'] == 'mul':\n",
    "    replacements = {'nyn': 'kin',\n",
    "                    'lgg': 'lin',\n",
    "                    'ach': 'tso',\n",
    "                    'teo': 'som',\n",
    "                    'luo': 'sna'}\n",
    "    for r in replacements:\n",
    "        if (f'>>{r}<<' not in tokenizer.encoder and\n",
    "            f'>>{replacements[r]}<<' in tokenizer.encoder):\n",
    "            tokenizer.encoder[f\">>{r}<<\"] = tokenizer.encoder[f\">>{replacements[r]}<<\"]\n",
    "            del tokenizer.encoder[f\">>{replacements[r]}<<\"]\n",
    "\n",
    "    # Check that all the evaluation language codes are mapped to something.\n",
    "    for r in config['eval_languages']:\n",
    "        if f'>>{r}<<' not in tokenizer.encoder:\n",
    "            raise ValueError(f'Language code {r} not found in the encoder.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9331d9b",
   "metadata": {},
   "source": [
    "Pre-process the raw text datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd05d31-b8e2-4049-bbb4-8abbeafacbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  = train_data_raw.map(\n",
    "    preprocess, remove_columns=[\"translation\"], batched=True)\n",
    "\n",
    "validation_data  = validation_data_raw.map(\n",
    "    preprocess, remove_columns=[\"translation\"], batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0ec59a",
   "metadata": {},
   "source": [
    "Launch the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb307074",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=config['wandb_project'], config=config)\n",
    "\n",
    "trainer = transformers.Seq2SeqTrainer(\n",
    "    model,\n",
    "    config['train_settings'],\n",
    "    train_dataset = train_data,\n",
    "    eval_dataset = validation_data,\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics = lambda x: compute_metrics(\n",
    "        x, config['eval_languages'], config['validation_samples_per_language']),\n",
    "    callbacks = [transformers.EarlyStoppingCallback(early_stopping_patience = 5)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f19723a-e5d2-4e02-bcc4-8b2a14c9fe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['eval_pretrained_model']:\n",
    "    trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2373b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bfc1e2-93b2-4bca-82d9-de1c23d9927e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
